\documentclass[titlepage]{article}

\usepackage{preamble}

\title{
    \textbf{COMPARISON OF \\
    HEURISTIC ALGORITHMS \\
    FOR KP01} \\[0.5cm]
    \rule{12cm}{0.3mm} \\[0.5cm] 
    \small \scshape{A comparison of heuristic algorithms for solving the 0-1 Knapsack Problem} \\[0.4cm]
    \rule{12cm}{0.3mm}
    \vskip -0.3cm
}

\author{
    \small \scshape{Raunak Redkar} \\ 
    \small \scshape{Supervisors: Per-Olof Freerks and Felicia Dinnetz} \\
    \scriptsize \scshape{Kungsholmens Gymnasium}
}

\begin{document}

\onehalfspacing

\maketitle

\newpage
\tableofcontents
\listofalgorithms
\newpage
\pagenumbering{arabic}

\section{Introduction}

\subsection{Background}

There are many situations in every day life, where a person wonders whether he is doing something efficiently. Unforunately, the human brain is not always capable of coming up with an optimal approach when the problem has a lot of factors at play. It is here when a system is used, and the true power of computing can be recognised. 

To solve such problems, a computer is provided all the information, from which it will produce an optimal answer. For the system to process all the information, certain instructions must be written into the system so that it knows how to handle the information. This set of instructions is called an algorithm. The system or computer uses algorithms to take in information (the input), and produce an answer - an output. The efficiency (in all aspects) of the algorithm is dependant on the instructions which make up it. There are many algorithms which have been designed to solve many problems. For problems in computer science, more than one algorithm is usually proposed and used. Optimization problems are where this is frequently the case. 

In the fields of computer science and mathematics, optimization problems are problems of finding the best solution, from a range of many feasible solutions. They are usually categorized into 2 categories: discrete optimizations and continuous optimizations, depending on whether the variables are discrete or continuous respectively. Combinatorial optimization problems are a subset of optimization problems that fall into the discrete. Combinatorial optimization involves searching for a maxima or minima for an objective function whose search space domain is a discrete but (usually large) space.

Typical combinatorial optimization problems are not limited to but include:
\begin{itemize}
    \item \textcolor{blue}{General Knapsack Problem} - Given a set of items, each with weight and profit value and a knapsack capacity, what is the best way to choose the items while respecting the knapsack capacity?
    \item \textcolor{blue}{Traveling Salesman Problem}- Given a list of cities, what is the shortest possible path that visits each city exactly once and returns to the origin?
    \item \textcolor{blue}{Set Cover} - Given a set of elements $\{1, 2, ..., n\}$ , what is the and a collection of $m$ sets whose union equals the universe, what is the smallest sub-collection of sets whose union is the universe?
\end{itemize}

Combinatorial optimization problems show up in an array of different fields. The Knapsack Problem in particular has many variants which include the 0-1 knapsack problem, the bounded and unbounded knapsack problems, the multidimensional knapsack problem, the discounted knapsack problem, etc. The 0-1 Knapsack Problem is the simplest form of the knapsack problem and thus has also been the main focus in the research community. It appears in real-world decision-making processes in a variety of fields. Some examples include: 

Many of combinatorial optimization problems including the 0-1 Knapsack problem currently do not have deterministic algorithms which are considered fast enough for them to be used on a large-scale basis. Consequently, the research focus has been on approaches that do not necessarily guarantee the best solution but win over deterministic approaches when it comes to time. 

\subsubsection*{Problem statement}
In the 0-1 Knapsack Problem, there $n$ items and a maximum weight capacity $W$. Each item has a profit value $p_i$ and a weight value $w_i$. Find the optimal selection of items which maximizes the profit value while respecting the max weight value. The problem can be mathematically represented as so:

\vskip -0.5cm

\begin{gather}
    \text{Maximize}\;\; f(\Vec{x}) = \sum_{i = 1}^{n} p_i x_i \\
    \text{subject to} \sum_{i = 1}^{n} w_i x_i <= W \\
    x \in \{0, 1\}
\end{gather}

The rest of this paper will follow the shorthand "KP01" for the 0-1 Knapsack Problem. 


\subsection{Aim}
The aim of this paper is to compare various algorithms in order to investigate the strength of commonly used techniques in for solving optimization problems.

\subsection{Research Question}
How do the global-best harmony search algorithm and the binary harmonic multi-scale algorithms perform when implemented for solving KP01?

\section{Theory}
\subsection{Computational Complexity}
In computer science, computational complexity is the measure of how expensive it is to the run an algorithm; the amount of resources required to run the algorithm. The 2 resources are time and memory, notated time complexity and space complexity. As the memory complexity is dependent on the time complexity, the time complexity is the limiting factor for the efficiency of an algorithm and is usually the one focused on. Computable problems can be factorial, exponential, polynomial, logarithmic, etc.

\subsubsection*{Big O notation}
The time complexity of an algorithm is a function of the size of the input of that algorithm. For example, if $n$ numbers $a_1, a_2, a_3, ..., a_n$ are given, and an algorithm checks the existence of an certain element in those numbers, by checking all n elements, then the time complexity is some constant times $n$ - the size of the input. Computer scientists would denote this using Big O notation as $\mathcal{O}(n)$. Big O notation is a system developed by mathematicians and computer scientists to describe an function/algorithm's asymptotic limiting factor.

Computer scientists usually categories the time complexity of an algorithm into polynomial time complexities and non polynomial time complexities. This is because non-polynomial functions like factorial and exponential functions tend to grow quicker than polynomial functions and thus are usually considered nonviable for large data. 

\vskip 1cm

% time complexities graph
\begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          grid = major,
          clip = true,
          ticks = none,
          width=1\textwidth,
          height=0.8\textwidth,
          every axis plot/.append style={very thick},
          axis line style = ultra thick,
          clip mode=individual,
          restrict y to domain=0:10,
          restrict x to domain=0:10,
          axis x line = left,
          axis y line = left,
          domain = 0.00:10,
          xmin = 0,
          xmax = 11,
          ymin = 0,
          ymax = 11,
          xlabel = n,
          ylabel = Computer Operations,
          xlabel style = {at={(axis description cs:0.5,-0.1)},anchor=south},
          ylabel style = {at={(axis description cs:-0.08,0.5)},anchor=north},
          label style = {font=\LARGE\bf},
          ]
          \addplot [
            samples=100, 
            color=red,
          ]
          {3*2^x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(3\cdot2^n)$};
          \addplot [
            samples=100,
            color=orange, 
          ] gnuplot{gamma(x+1)} node[above,pos=1,style={font=\Large}]{$\mathcal{O}(n!)$};
          \addplot [
            samples=100, 
            color=green,
          ]
          {x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(n)$};
          \addplot [
            samples=100, 
            color=blue,
          ]
          {log2 x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(\log{}n)$};
          \addplot [
            samples=100,
            color=violet,
          ]
          {2*sqrt x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(2\sqrt{n})$};
          \addplot [
          samples=100, 
          color=red,
          ]
          {1}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(1)$};  
            
      \end{axis}
    \end{tikzpicture}
\end{center}

\paragraph{Solving the 0-1 Knapsack Problem}\mbox{}\

There are 2 main approaches to solving KP01: An exact solution using deterministic algorithms, and probabilistic approaches involving heuristic algorithms. A small-scale KP01 can be solved with deterministic approaches, but for high-scale situations it is not realistic to get optimal solutions with exact approaches \ref{QWPA} as the KP01 problem is NP-complete \ref{some website}.

\subsection*{Deterministic algorithms}
Dynamic programming is a general technique for solving optimization problems. If a problem has optimal substructure and over-lapping subproblems, then dynamic programming is applicable. In computer science 
a problem has \emph{optimal substructure} if an optimal solution for a problems can be constructed from optimal solutions of its sub-problems, and \emph{over-lapping subproblems} is when a problem can be decomposed into sub-problems which are reused. Dynamic programming breaks down a complicate problem into smaller sub-problems in a recursive manner, while also using some memory to save the solutions of the sub-problems (usually in tabular form). This way, when we need to get a solution for a sub-problem again, we can just use our previously calculated value. 

KP01 is solved using dynamic programming using a table based computation:

%dp
\begin{breakablealgorithm}
\caption{Solving 0-1 Knapsack with Dynamic Programming}\label{dp}
    \begin{algorithmic}[1]
        \For {$i = 0 \text{ to } noItems $} \Comment{\textcolor{blue}{If no items, then profit = 0}}
            \State $Table[i][0] \gets 0 $ 
        \EndFor
        \For {$k = 0 \text{ to } maxCapacity $} \Comment{\textcolor{blue}{If no capacity, then profit = 0}}
            \State $Table[0][k] \gets 0 $
        \EndFor
        \For {$i = 0 \text{ to } noItems$}
            \For {$k = 0 \text{ to } maxCapacity $}
                \State $Table[i][k] \gets Table[i-1][k]$  
                \If {$k-weights[i] >= 0$} 
                    \State $Table[i][k] \gets max(Table[i-1][k-weights[i]] + profits[i], Table[i][k]) $
                \EndIf
            \EndFor
        \EndFor
        \State $\textbf{Print } Table[noItems][maxCapacity] $
    \end{algorithmic}
\end{breakablealgorithm}

\vskip 0.5cm

This implementation of dynamic programming method has a time complexity of $\mathcal{O}(N\cdot W)$, where N is the number of items, and W is max capacity. The dynamic programming algorithm does not end until the entire table is built. This proves to be inefficient very quickly as the maximum capacity and the number of items in the knapsack increases. So with the increase in the scale, the feasibility of dynamic programming decreases which has incentivized research in the field.

\subsection*{Heuristic algorithms}
When deterministic algorithms prove to be too slow for practical applications, heuristic algorithms are chosen for their (usually) near optimal outputs and their speed. Heuristic algorithms should not be confused with approximation algorithms. Approximation algorithms guarantee a maximum margin of error; a constant factor off of the optimal. On the other hand, a heuristic algorithm does not guarantee anything, so it can perform better or worse than an approximation algorithm. 

Heuristic algorithms for combinatorial optimization are usually designed in a specific way \ref{some website}:
\begin{enumerate}
    \item Create a potential solution which is current best.
    \item Generate new solution. (usually in polynomial time) 
    \item If new solution better than current best, swap out the best with the newly generated. 
    \item If satisfied, output best solution, else continue.
\end{enumerate}

Both the Discrete Global-Best Harmony Search (DGHS), and Binary Multi-Scale Quantum Harmonic Oscillator (BMQHOA) have a similar framework.

\subsubsection{Discrete Global-Best Harmony Search Algorithm}
The Harmony Search algorithm (HS) was developed in 2001 \ref{geem et al} based on the improvisation of music players \textcolor{red}{EXPAND}. This algorithm was made for continuous search spaces and thus cannot be used for discrete search spaces in combinatorial optimization problems. The Discrete Global-Best harmony search was proposed by \textcolor{red}{EXPAND} to overcome this. The DGHS for KP01 also has a repair-operator and a greedy selection mechanism. A harmony in this algorithm refers to a candidate solution, or a certain selection of items to be put in the knapsack. 

It consists of 4 main parts:
\begin{enumerate}
    \item Create a harmony memory HM - a fixed size (HMS) of randomly generated harmonies. Initialize parameters harmony memory consider rate (HMCR), and pitch adjusting rate (PAR), and calculate profit-density vector for usage in the repair-operator.
    \item Create a new harmony using the current HM \ref{harmonyGen}.
    \item If generated harmony is better than the best in the harmony, replace it. If not, check if it is better than then worst harmony, and replace it if so. 
    \item If maximum number of iterations has been met, output the best sum profit value found so far.
\end{enumerate}

\vskip 0.5cm
%DGHS
\begin{breakablealgorithm}
\caption{The DGHS algorithm}\label{DGHS}
    \begin{algorithmic}[1]
        \State Set the harmony memory size $HMS$, the number of iterations $ITERATIONS$, and the minimum and maximum values of parameters $PAR$ and $HMCR$.
        \State Initialize the HM through a randomized process, and use \ref{harmonyRepair} to the generated harmonies. Calculate the totalProfit and totalWeight values for each harmony in HM.
        \State $iterator \gets 1$
        \While {$iterator <= ITERATIONS$}
            \State Record indices of the best and the worst harmonies in HM.
            \State Calculate parameters HMCR and PAR for the current iteration.
            \State Perform Algorithm \ref{harmonyGen} to produce a new harmony $\Vec{x}_{new}$
            \State Perform Algorithm \ref{harmonyRepair} to repair the new harmony $\Vec{x}_{new}$
            \If {$\Vec{x}_{new} \text{ is better than or equal to } \Vec{x}_{best}$}
                \State Replace $\Vec{x}_{best} \text{ with } \Vec{x}_{new}$
            \ElsIf {$\Vec{x}_{new} \text{ is better than or equal to } \Vec{x}_{worst}$}
                \State Replace $\Vec{x}_{worst} \text{ with } \Vec{x}_{new}$
            \EndIf
            \State $iterator \gets iterator+1$
        \EndWhile
        \State Output profit of best harmony
    \end{algorithmic}
\end{breakablealgorithm}
\vskip 0.5cm

The parameters HMCR and PAR are used when generating new harmonies. They determine the likelihood of heading toward the current best harmony, and the likelihood of randomly flipping a decision variable. The idea is that current best harmony should have an influence on the generation of a new harmony, and that a decision variable (whether or not an item is put in the knapsack) should be flipped to allow for diversity in the HM, making it easier to overcome local maxima. These parameters are determined in terms of the current iteration like so:

\begin{equation}
    HMCR(t) = HMCR_{max} - \frac{HMCR_{max}-HMCR_{min}}{ITERATIONS} t
\end{equation}
\begin{equation}
    PAR(t) = PAR_{max} - \frac{PAR_{max}-PAR_{min}}{ITERATIONS} t
\end{equation}

The dynamic updating of the parameters is designed in this way to let larger values of HMCR help accelerate the convergence of the harmonies early on with the the help of the best individual harmony, while smaller values of HMCR can help you overcome local maxima \ref{DGHS-algorithm}. Similarly, larger values of PAR in the beginning allow for increases in diversity through mutations (item decision bit flips), when there is time to "explore" different variants, smaller values allow convergence at the end of the search. 

With these parameters one generates a new harmony:

%harmonyGen
\begin{breakablealgorithm}
\caption{Generating a new harmony during iterative part (part 2)}\label{harmonyGen}
    \begin{algorithmic}[1]
        \For {$i = 1 \text{ to } noItems$}
            \If {$rand(0, 1) <= HMCR(t)$}
                \State $newHarmony[i] \gets bestHarmony[i]$ 
            \Else
                \State Generate a random integer number $a \in \{1, 2, ... HMS\}, a \neq best$
                \State $newHarmony[i] \gets Harmony_{a}[i]$ 
                \If {$rand(0, 1) <= PAR(t)$}
                    \State $newHarmony[i] = |newHarmony[i]-1|$ \Comment{\textcolor{blue}{Flipping i'th item - mutation}}
                \EndIf
            \EndIf
        \EndFor
    \end{algorithmic}
\end{breakablealgorithm}
\vskip 0.5cm


Any time a harmony is generated, during Part 1 or Part 2, it is "repaired" in case the selection of items exceeds the capacity of the knapsack. The new generated harmony is also repaired if it can fit more items, without exceeding the capacity of the knapsack. The repair-operator consists of 2 phases:
\begin{enumerate}
    \item Drop phase - repairing a harmony if it violates the constraint
    \item Add phase - adding a items into the knapsack if the total weight is less than the capacity
\end{enumerate}

The add phase is always done after the drop phase, as a harmony previously infeasible becomes feasible after the drop phase, but its total weight may be less than the capacity of the knapsack. Here is the pseudocode for the repair-operator:

\vskip 0.5cm
%harmonyRepair
\begin{breakablealgorithm}
\caption{Repair-operator for DGHS}\label{harmonyRepair}
    \begin{algorithmic}[1]
        \If {$totalWeight > maxWeight$}
            \For {$i = 1 \text{ to } N$} \Comment{\textcolor{blue}{DROP phase}}
                \State $\lambda_{i} = \frac{profit_{i}}{weight_{i}}$
            \EndFor
            \State Sort items in increasing order of $\lambda_{i}$, and let $ind_{i}$ denote the original index of each $\lambda_{i}$
            \For {$i = 1 \text{ to } noItems $} 
            \State Remove the ones with the least profit-density values greedily
                \If {$\lambda_{i} == 0$}
                    \State \textbf{Continue}
                \EndIf
                \State $newHarmony[ind_{i}] = 0$ \Comment{\textcolor{blue}{Unload the item}}
                \State $totalWeight \gets totalWeight - weight[ind_{i}]$
                \State $totalProfit \gets totalProfit - profit[ind_{i}]$
                \If {$totalWeight <= maxWeight$}
                    \State \textbf{Break} \Comment{\textcolor{blue}{Terminate DROP phase}}
                \EndIf
            \EndFor
        \EndIf
        \If {$totalWeight < maxWeight$}
            \For {$i = 1 \text{ to } N$} \Comment{\textcolor{blue}{ADD phase}}
                \State $\lambda_{i} = \frac{profit_{i}}{weight_{i}}$
            \EndFor
            \State Sort items in increasing order of $\lambda_{i}$, and let $ind_{i}$ denote the original index of each $\lambda_{i}$
            \For {$i = 1 \text{ to } noItems $} 
                \State Add the ones with the greatest profit-density if possible 
                \If {$newHarmony[ind_{i}] == 0$}
                    \If {$totalWeight + weight[ind_{i}] <= maxWeight$}
                        \State $newHarmony[ind_{i}] = 1$
                        \State $totalWeight \gets totalWeight + weight[ind_{i}]$
                        \State $totalProfit \gets totalProfit + profit[ind_{i}]$
                    \EndIf
                \EndIf
            \EndFor
        \EndIf
    \end{algorithmic}
\end{breakablealgorithm}

\vskip 1cm
\subsubsection{Binary Multi-Scale Quantum Harmonic Oscillator Algorithm}

The Multi-Scale Quantum Harmonic Oscillator Algorithm (MQHOA) is called such as it follows a model of a solving a particle's ground state wave function under the harmonic oscillator potential well \ref{BMQHOA-article} . In MQHOA, candidate solutions are generated by sampling points in a Gaussian distribution within a certain distance of something. The Binary Multi-Scale Quantum Harmonic Oscillator Algorithm (BMQHOA) discretizes this by defining the number of bits between solutions (item decision variable ) as the distance between solutions, so it becomes a discrete search space. Similar to the DGHS algorithm, a repair-operator is added fix solutions which violate the capacity constraint. 

The BMQHOA algorithm consists of 4 parts:
\begin{enumerate}
    \item Random binary vector generation -> repair
    \item Generating a solutions by flipping $m$ items, mutating, and repairing
    \item Reducing the standard deviation value for the normal distribution from which $m$ is determined every iteration.
    \item If termination constraint is satisfied, output best totalProfit value, else return to Step 2.
\end{enumerate}

%BMQHOA
\begin{breakablealgorithm}
\caption{The BMQHOA algorithm with solution generation}\label{BMQHOA}
    \begin{algorithmic}[1]
        \State Set the number of iterations $ITERATIONS$ and the number of binary vectors (solutions) - BINVEC in memory.
        \State Randomly generate the binary vectors and use Algorithm \ref{harmonicRepair} to repair the vectors. 
        \While {$iterator <= ITERATIONS$}
            \State Update $\sigma_{s}$
            \State $found \gets FALSE$
            \While {$found == FALSE$}
                \State Try to generate a solution
                \State Let $solutions_{new}$ be the new generated vector 
                \State $solutions_{new} \gets solutions_{best}$
                \State Generate the number of flipped bits $m ~ N(0, \sigma_{s})$
                \State Treat $solutions_{new}$ as a circular array
                \State Randomly select a position in $solutions_{new}$ and flip the next $m$ items.
                \State Mutate a random bit towards current best solution (flip an item)
                \State $solutions_{new}[rand] \gets solutions_{best}[rand]$
                \State Repair newly generated solution
                \If {$totalProfit >= totalProfit_{worst}$}
                    \State Replace worst solution with newly generated solution
                    \State $solutions_{worst} \gets solutions_{new}$
                    \State $found = TRUE$
                \EndIf
            \EndWhile
        \EndWhile
    \end{algorithmic}
\end{breakablealgorithm}
\vskip 0.5cm

After generating a new solution, $m$ items are flipped in the hope of increasing diversity in the solutions and increasing the likelihood of finding the optimal solution. They are generated randomly with a normal probability distribution around \textbf{0}, with a mean of $0$ and a variable std deviation. The Std. Deviation decreases with each new-solution-generation iteration so as to increase diversity in the beginning of the search, while reducing the likelihood of corrupting a binary vector closer to the end of the search when close to optimal solutions have (hopefully) been found. Initially the value of the standard deviation is set to $noItems/3$, so that there is a $\sim 99.7\%$ chance that the generated number of flipped items $m$ are in the range $[0-3\sigma_{s}, 0+3\sigma_{s}]$:
$$STDEV_{max} = noItems/3$$
$$\sigma_{s} = 1-\frac{t}{ITERATIONS} STDEV_{max}$$


The normal distribution probability density function generator is implemented with the C++ Standard Library class normal\_distribution. An item's decision value is also made to match the corresponding decision in the best solution, which gives a slow mutation towards the current best solution allowing for diversity while still allowing the best solution to influence the process. Allowing multiple bits to mutate toward current best, this can lead to a premature local maximum, nullifying the rest of the search \ref{BMQHOA-article} . 

The repair-operator of the BMQHOA algorithm has 3 phases:
\begin{enumerate}
    \item Density-first stage: The already selected items are sorted based on their profit to weight ratio in non-increasing order and then greedily selected while respecting the weight constraint. 
    \item Minimum-weight-first stage: Out of the items that weren't selected in the first stage are sorted based on their weight values in non-decreasing order, then greedily selected while respecting the weight constraint.   
\end{enumerate}

$Q_{1}[1, 2,...,n]$ is the index for the items in the density ratio array in the original vector. $Q_{2}[1, 2,..., n]$ is index of the items in the minimum weight sorted array in the original vector.

\begin{breakablealgorithm}
\caption{Repair-Operator for BMQHOA}\label{harmonicRepair}
    \begin{algorithmic}
        \State Let $x$ be the current array/vector.
        \State $totalWeight = 0, temp = 0, i = 0, b = 0$
        \State Stage 1: Density first stage
        \While {$temp < maxWeight$}
            \State $totalWeight = temp$
            \State $i += 1$
            \State $temp = temp + weight[Q_{1}[i]]$
        \EndWhile
        \For {$j = i \text{ to } n$}
            \State $x[Q_{1}[j]] = 0$
            \State $totalWeight += x[Q_{1}[j]]$
        \EndFor
        \State Stage 2: Minimum weight first stage
        \While {$totalWeight < C$}
            \State $b += 1$
            \If {$totalWeight + weight[Q_{2}[b]] <= C$} 
                    \State $x[Q_{2}[b]] = 1$
                \State $totalWeight += weight[Q_{2}[b]]$
            \EndIf
        \EndWhile
    \end{algorithmic}
\end{breakablealgorithm}

\section{Methodology}

Testdata is made up of different testcases, each of which is an input on which every algorithm is run.

\subsection{Generating testdata}

Five testcases were taken from a database \ref{some website}. These five testcases are mentioned/used several times in the literature.  Most of the testcases were made using a random number generator, where a random number was generated using the Mersenne Prime Twister (mt19937). A computer cannot generate a truly random number. A pseudorandom number is what is generated, which is a number which appears to be statistically random, but has been generated using a deterministic process. 

First, the number of solutions in memory (for the heuristic algorithms) was put in the input file which was kept constant, 20, in all testcases and algorithms for this report. In all testcases, the number of items $n$ was chosen to either be 1000 or 100000. The profit and weight values were then randomly generated in a range of \, $ [1, n] $ while a sum-of-weights variable \emph{sum} was kept. The max weight was then randomly generated in a range of $[\frac{\text{sum}}{100} , \frac{\text{sum}}{10}]$, so that the knapsack wouldn't be able to hold all the items. To avoid any overflow errors, all generated numbers (and the sum of the profit/weight values) were kept below the max value of an \emph{int} which is $2^{31}-1$ in many programming languages.  Then $n$, $W$ (maxWeight), and each item (profit and weight values) was put in an input file. 

These input files were then saved for later testing of the algorithms. 

Each testcase was then run on each algorithm and saved in a spreadsheet:



 \bibliography{refs}

\end{document}
