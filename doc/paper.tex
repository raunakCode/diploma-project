\documentclass[titlepage]{article}

\usepackage{preamble}

\title{
    \textbf{COMPARISON OF \\
    HEURISTIC ALGORITHMS \\
    FOR KP01} \\[0.5cm]
    \rule{12cm}{0.3mm} \\[0.5cm] 
    \small \scshape{A comparison of heuristic algorithms for solving the 0-1 Knapsack Problem} \\[0.4cm]
    \rule{12cm}{0.3mm}
    \vskip -0.3cm
}

\author{
    \small \scshape{Raunak Redkar} \\ 
    \small \scshape{Supervisors: Per-Olof Freerks and Felicia Dinnetz} \\
    \scriptsize \scshape{Kungsholmens Gymnasium}
}

\begin{document}

\onehalfspacing

\maketitle

\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}

\subsection{Background}

There are many situations in every day life, where a person wonders whether he is doing something efficiently. Unforunately, the human brain is not always capable of coming up with an optimal approach when the problem has a lot of factors at play. It is here when a system is used, and the true power of computing can be recognised. 

To solve such problems, a computer is provided all the information, from which it will produce an optimal answer. For the system to process all the information, certain instructions must be written into the system so that it knows how to handle the information. This set of instructions is called an algorithm. The system or computer uses algorithms to take in information (the input), and produce an answer - an output. The efficiency (in all aspects) of the algorithm is dependant on the instructions which make up it. There are many algorithms which have been designed to solve many problems. For problems in computer science, more than one algorithm is usually proposed and used. Optimization problems are where this is frequently the case. 

In the fields of computer science and mathematics, optimization problems are problems of finding the best solution, from a range of many feasible solutions. They are usually categorized into 2 categories: discrete optimizations and continuous optimizations, depending on whether the variables are discrete or continuous respectively. Combinatorial optimization problems are a subset of optimization problems that fall into the discrete. Combinatorial optimization involves searching for a maxima or minima for an objective function whose search space domain is a discrete but (usually large) space.

Typical combinatorial optimization problems are not limited to but include:
\begin{itemize}
    \item \textcolor{blue}{General Knapsack Problem} - Given a set of items, each with weight and profit value and a knapsack capacity, what is the best way to choose the items while respecting the knapsack capacity?
    \item \textcolor{blue}{Traveling Salesman Problem}- Given a list of cities, what is the shortest possible path that visits each city exactly once and returns to the origin?
    \item \textcolor{blue}{Set Cover} - Given a set of elements $\{1, 2, ..., n\}$ , what is the and a collection of $m$ sets whose union equals the universe, what is the smallest sub-collection of sets whose union is the universe?
\end{itemize}

Combinatorial optimization problems show up in an array of different fields. The Knapsack Problem in particular has many variants which include the 0-1 knapsack problem, the bounded and unbounded knapsack problems, the multidimensional knapsack problem, the discounted knapsack problem, etc. The 0-1 Knapsack Problem is the simplest form of the knapsack problem and thus has also been the main focus in the research community. It appears in real-world decision-making processes in a variety of fields. Some examples include: 

Many of combinatorial optimization problems including the 0-1 Knapsack problem currently do not have deterministic algorithms which are considered fast enough for them to be used on a large-scale basis. Consequently, the research focus has been on approaches that do not necessarily guarantee the best solution but win over deterministic approaches when it comes to time. 

\subsubsection*{Problem statement}
In the 0-1 Knapsack Problem, there $n$ items and a maximum weight capacity $W$. Each item has a profit value $p_i$ and a weight value $w_i$. Find the optimal selection of items which maximizes the profit value while respecting the max weight value. The problem can be mathematically represented as so:

\vskip -0.5cm

\begin{gather}
    \text{Maximize}\;\; f(\Vec{x}) = \sum_{i = 1}^{n} p_i x_i \\
    \text{subject to} \sum_{i = 1}^{n} w_i x_i <= W \\
    x \in \{0, 1\}
\end{gather}

The rest of this paper will follow the shorthand "KP01" for the 0-1 Knapsack Problem. 


\subsection{Aim}
The aim of this paper is to compare various algorithms in order to investigate the strength of commonly used techniques in for solving optimization problems.

\subsection{Research Question}
How do the global-best harmony search algorithm and the binary harmonic multi-scale algorithms perform when implemented for solving KP01?

\section{Theory}
\subsection{Computational Complexity}
In computer science, computational complexity is the measure of how expensive it is to the run an algorithm; the amount of resources required to run the algorithm. The 2 resources are time and memory, notated time complexity and space complexity. As the memory complexity is dependent on the time complexity, the time complexity is the limiting factor for the efficiency of an algorithm and is usually the one focused on. Computable problems can be factorial, exponential, polynomial, logarithmic, etc.

\subsubsection*{Big O notation}
The time complexity of an algorithm is a function of the size of the input of that algorithm. For example, if $n$ numbers $a_1, a_2, a_3, ..., a_n$ are given, and an algorithm checks the existence of an certain element in those numbers, by checking all n elements, then the time complexity is some constant times $n$ - the size of the input. Computer scientists would denote this using Big O notation as $\mathcal{O}(n)$. Big O notation is a system developed by mathematicians and computer scientists to describe an function/algorithm's asymptotic limiting factor.

Computer scientists usually categories the time complexity of an algorithm into polynomial time complexities and non polynomial time complexities. This is because non-polynomial functions like factorial and exponential functions tend to grow quicker than polynomial functions and thus are usually considered nonviable for large data. 

\vskip 1cm

% time complexities graph
\begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          grid = major,
          clip = true,
          ticks = none,
          width=1\textwidth,
          height=0.8\textwidth,
          every axis plot/.append style={very thick},
          axis line style = ultra thick,
          clip mode=individual,
          restrict y to domain=0:10,
          restrict x to domain=0:10,
          axis x line = left,
          axis y line = left,
          domain = 0.00:10,
          xmin = 0,
          xmax = 11,
          ymin = 0,
          ymax = 11,
          xlabel = n,
          ylabel = Computer Operations,
          xlabel style = {at={(axis description cs:0.5,-0.1)},anchor=south},
          ylabel style = {at={(axis description cs:-0.08,0.5)},anchor=north},
          label style = {font=\LARGE\bf},
          ]
          \addplot [
          samples=100, 
          color=green,
          ]
          {x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(n)$};
          \addplot [
          samples=100, 
          color=blue,
          ]
          {log2 x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(\log{}n)$};
          \addplot [
          samples=100, 
          color=red,
          ]
          {3*2^x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(3\cdot2^n)$};
          \addplot [
          samples=100, 
          color=violet,
          ]
          {1}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(1)$};  
          \addplot [
          samples=100,
          color=orange, 
          ] gnuplot{gamma(x+1)} node[above,pos=1,style={font=\Large}]{$\mathcal{O}(n!)$};  
      \end{axis}
    \end{tikzpicture}
\end{center}

\subsection{Solving the 0-1 Knapsack Problem}

There are 2 main approaches to solving KP01: An exact solution using deterministic algorithms, and probabilistic approaches involving heuristic algorithms. A small-scale KP01 can be solved with deterministic approaches, but for high-scale situations it is not realistic to get optimal solutions with exact approaches \ref{QWPA} as the KP01 problem is NP-complete \ref{some website}.

\subsubsection*{Deterministic algorithms}
Dynamic programming is a general technique for solving optimization problems. If a problem has optimal substructure and over-lapping subproblems, then dynamic programming is applicable. In computer science 
a problem has \emph{optimal substructure} if an optimal solution for a problems can be constructed from optimal solutions of its sub-problems, and \emph{over-lapping subproblems} is when a problem can be decomposed into sub-problems which are reused. Dynamic programming breaks down a complicate problem into smaller sub-problems in a recursive manner, while also using some memory to save the solutions of the sub-problems (either in tabular form or memoized form). This way, when we need to get a solution for a sub-problem again, we can just use our previously calculated value. 

KP01 is solved using dynamic programming using a table based computation:

\begin{algorithm}
\caption{Pseudocode for solving 0-1 Knapsack with Dynamic Programming}\label{dp}
    \begin{algorithmic}
        \For {$i = 0 \text{ to } noItems $} \Comment{// If no items, then profit = 0}
            \State $Table[i][0] \gets 0 $ 
        \EndFor
        \For {$k = 0 \text{ to } maxCapacity $} \Comment{// If no items, then profit = 0}
            \State $Table[0][k] \gets 0 $
        \EndFor
        \For {$i = 0 \text{ to } noItems$}
            \For {$k = 0 \text{ to } maxCapacity $}
                \State $Table[i][k] \gets Table[i-1][k]$  
                \If {$k-weights[i] >= 0$} 
                    \State $Table[i][k] \gets max(Table[i-1][k-weights[i]] + profits[i], Table[i][k]) $
                \EndIf
            \EndFor
        \EndFor
        \State $\textbf{Print } Table[noItems][maxCapacity] $
    \end{algorithmic}
\end{algorithm}

This implementation of dynamic programming method has a time complexity of $\mathcal{O}(N\cdot W)$, where N is the number of items, and W is max capacity. The dynamic programming algorithm does not end until the entire table is built. This proves to be inefficient very quickly as the maximum capacity of the knapsack increases. So with the increase in the scale, the feasibility of dynamic programming decreases.

\subsubsection*{Heuristic algorithms}
When deterministic algorithms prove to be too slow for practical applications, heuristic algorithms are chosen for their (usually) near optimal outputs and their speed. Heuristic algorithms should not be confused with approximation algorithms. Approximation algorithms guarantee a maximum margin of error; a constant factor off of the optimal. On the other hand, a heuristic algorithm does not guarantee anything, so it can perform better or worse than an approximation algorithm. 

\paragraph*{Discrete Global-Best Harmony Search Algorithm}
\paragraph*{Binary Multi-Scale Quantum Harmonic Oscillator Algorithm}






 \bibliography{refs}

\end{document}
