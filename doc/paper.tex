\documentclass[titlepage]{article}

\usepackage{preamble}

\title{
    \textbf{COMPARISON OF \\
    HEURISTIC ALGORITHMS \\
    FOR KP01} \\[0.5cm]
    \rule{12cm}{0.3mm} \\[0.5cm] 
    \small \scshape{A comparison of heuristic algorithms for solving the 0-1 Knapsack Problem} \\[0.4cm]
    \rule{12cm}{0.3mm}
    \vskip -0.3cm
}

\author{
    \small \scshape{Raunak Redkar} \\ 
    \small \scshape{Supervisors: Per-Olof Freerks and Felicia Dinnetz} \\
    \scriptsize \scshape{Kungsholmens Gymnasium}
}

\begin{document}

\onehalfspacing

\maketitle

\newpage
\tableofcontents
\listofalgorithms
\newpage
\pagenumbering{arabic}

\section{Introduction}

\subsection{Background}

There are many situations in every day life, where a person wonders whether he is doing something efficiently. Unforunately, the human brain is not always capable of coming up with an optimal approach when the problem has a lot of factors at play. It is here when a system is used, and the true power of computing can be recognised. 

To solve such problems, a computer is provided all the information, from which it will produce an optimal answer. For the system to process all the information, certain instructions must be written into the system so that it knows how to handle the information. This set of instructions is called an algorithm. The system or computer uses algorithms to take in information (the input), and produce an answer - an output. The efficiency (in all aspects) of the algorithm is dependant on the instructions which make up it. There are many algorithms which have been designed to solve many problems. For problems in computer science, more than one algorithm is usually proposed and used. Optimization problems are where this is frequently the case. 

In the fields of computer science and mathematics, optimization problems are problems of finding the best solution, from a range of many feasible solutions. They are usually categorized into 2 categories: discrete optimizations and continuous optimizations, depending on whether the variables are discrete or continuous respectively. Combinatorial optimization problems are a subset of optimization problems that fall into the discrete. Combinatorial optimization involves searching for a maxima or minima for an objective function whose search space domain is a discrete but (usually large) space.

Typical combinatorial optimization problems are not limited to but include:
\begin{itemize}
    \item \textcolor{blue}{General Knapsack Problem} - Given a set of items, each with weight and profit value and a knapsack capacity, what is the best way to choose the items while respecting the knapsack capacity?
    \item \textcolor{blue}{Traveling Salesman Problem}- Given a list of cities, what is the shortest possible path that visits each city exactly once and returns to the origin?
    \item \textcolor{blue}{Set Cover} - Given a set of elements $\{1, 2, ..., n\}$ , what is the and a collection of $m$ sets whose union equals the universe, what is the smallest sub-collection of sets whose union is the universe?
\end{itemize}

Combinatorial optimization problems show up in an array of different fields. The Knapsack Problem in particular has many variants which include the 0-1 knapsack problem, the bounded and unbounded knapsack problems, the multidimensional knapsack problem, the discounted knapsack problem, etc. The 0-1 Knapsack Problem is the simplest form of the knapsack problem and thus has also been the main focus in the research community. It appears in real-world decision-making processes in a variety of fields. Some examples include: 

Many of combinatorial optimization problems including the 0-1 Knapsack problem currently do not have deterministic algorithms which are considered fast enough for them to be used on a large-scale basis. Consequently, the research focus has been on approaches that do not necessarily guarantee the best solution but win over deterministic approaches when it comes to time. 

\subsubsection*{Problem statement}
In the 0-1 Knapsack Problem, there $n$ items and a maximum weight capacity $W$. Each item has a profit value $p_i$ and a weight value $w_i$. Find the optimal selection of items which maximizes the profit value while respecting the max weight value. The problem can be mathematically represented as so:

\vskip -0.5cm

\begin{gather}
    \text{Maximize}\;\; f(\Vec{x}) = \sum_{i = 1}^{n} p_i x_i \\
    \text{subject to} \sum_{i = 1}^{n} w_i x_i <= W \\
    x \in \{0, 1\}
\end{gather}

The rest of this paper will follow the shorthand "KP01" for the 0-1 Knapsack Problem. 


\subsection{Aim}
The aim of this paper is to compare various algorithms in order to investigate the strength of commonly used techniques in for solving optimization problems.

\subsection{Research Question}
How do the global-best harmony search algorithm and the binary harmonic multi-scale algorithms perform when implemented for solving KP01?

\section{Theory}
\subsection{Computational Complexity}
In computer science, computational complexity is the measure of how expensive it is to the run an algorithm; the amount of resources required to run the algorithm. The 2 resources are time and memory, notated time complexity and space complexity. As the memory complexity is dependent on the time complexity, the time complexity is the limiting factor for the efficiency of an algorithm and is usually the one focused on. Computable problems can be factorial, exponential, polynomial, logarithmic, etc.

\subsubsection*{Big O notation}
The time complexity of an algorithm is a function of the size of the input of that algorithm. For example, if $n$ numbers $a_1, a_2, a_3, ..., a_n$ are given, and an algorithm checks the existence of an certain element in those numbers, by checking all n elements, then the time complexity is some constant times $n$ - the size of the input. Computer scientists would denote this using Big O notation as $\mathcal{O}(n)$. Big O notation is a system developed by mathematicians and computer scientists to describe an function/algorithm's asymptotic limiting factor.

Computer scientists usually categories the time complexity of an algorithm into polynomial time complexities and non polynomial time complexities. This is because non-polynomial functions like factorial and exponential functions tend to grow quicker than polynomial functions and thus are usually considered nonviable for large data. 

\vskip 1cm

% time complexities graph
\begin{center}
    \begin{tikzpicture}
      \begin{axis}[
          grid = major,
          clip = true,
          ticks = none,
          width=1\textwidth,
          height=0.8\textwidth,
          every axis plot/.append style={very thick},
          axis line style = ultra thick,
          clip mode=individual,
          restrict y to domain=0:10,
          restrict x to domain=0:10,
          axis x line = left,
          axis y line = left,
          domain = 0.00:10,
          xmin = 0,
          xmax = 11,
          ymin = 0,
          ymax = 11,
          xlabel = n,
          ylabel = Computer Operations,
          xlabel style = {at={(axis description cs:0.5,-0.1)},anchor=south},
          ylabel style = {at={(axis description cs:-0.08,0.5)},anchor=north},
          label style = {font=\LARGE\bf},
          ]
          \addplot [
          samples=100, 
          color=green,
          ]
          {x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(n)$};
          \addplot [
          samples=100, 
          color=blue,
          ]
          {log2 x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(\log{}n)$};
          \addplot [
          samples=100, 
          color=red,
          ]
          {3*2^x}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(3\cdot2^n)$};
          \addplot [
          samples=100, 
          color=violet,
          ]
          {1}node[above,pos=1,style={font=\Large}]{$\mathcal{O}(1)$};  
          \addplot [
          samples=100,
          color=orange, 
          ] gnuplot{gamma(x+1)} node[above,pos=1,style={font=\Large}]{$\mathcal{O}(n!)$};  
      \end{axis}
    \end{tikzpicture}
\end{center}

\subsection{Solving the 0-1 Knapsack Problem}

There are 2 main approaches to solving KP01: An exact solution using deterministic algorithms, and probabilistic approaches involving heuristic algorithms. A small-scale KP01 can be solved with deterministic approaches, but for high-scale situations it is not realistic to get optimal solutions with exact approaches \ref{QWPA} as the KP01 problem is NP-complete \ref{some website}.

\subsubsection*{Deterministic algorithms}
Dynamic programming is a general technique for solving optimization problems. If a problem has optimal substructure and over-lapping subproblems, then dynamic programming is applicable. In computer science 
a problem has \emph{optimal substructure} if an optimal solution for a problems can be constructed from optimal solutions of its sub-problems, and \emph{over-lapping subproblems} is when a problem can be decomposed into sub-problems which are reused. Dynamic programming breaks down a complicate problem into smaller sub-problems in a recursive manner, while also using some memory to save the solutions of the sub-problems (either in tabular form or memoized form). This way, when we need to get a solution for a sub-problem again, we can just use our previously calculated value. 

KP01 is solved using dynamic programming using a table based computation:

\begin{algorithm}
\caption{Algorithm 2: Solving 0-1 Knapsack with Dynamic Programming}\label{dp}
    \begin{algorithmic}
        \For {$i = 0 \text{ to } noItems $} \Comment{// If no items, then profit = 0}
            \State $Table[i][0] \gets 0 $ 
        \EndFor
        \For {$k = 0 \text{ to } maxCapacity $} \Comment{// If no capacity, then profit = 0}
            \State $Table[0][k] \gets 0 $
        \EndFor
        \For {$i = 0 \text{ to } noItems$}
            \For {$k = 0 \text{ to } maxCapacity $}
                \State $Table[i][k] \gets Table[i-1][k]$  
                \If {$k-weights[i] >= 0$} 
                    \State $Table[i][k] \gets max(Table[i-1][k-weights[i]] + profits[i], Table[i][k]) $
                \EndIf
            \EndFor
        \EndFor
        \State $\textbf{Print } Table[noItems][maxCapacity] $
    \end{algorithmic}
\end{algorithm}

This implementation of dynamic programming method has a time complexity of $\mathcal{O}(N\cdot W)$, where N is the number of items, and W is max capacity. The dynamic programming algorithm does not end until the entire table is built. This proves to be inefficient very quickly as the maximum capacity and the number of items in the knapsack increases. So with the increase in the scale, the feasibility of dynamic programming decreases which has incentivized research in the field.

\subsubsection*{Heuristic algorithms}
When deterministic algorithms prove to be too slow for practical applications, heuristic algorithms are chosen for their (usually) near optimal outputs and their speed. Heuristic algorithms should not be confused with approximation algorithms. Approximation algorithms guarantee a maximum margin of error; a constant factor off of the optimal. On the other hand, a heuristic algorithm does not guarantee anything, so it can perform better or worse than an approximation algorithm. 

Heuristic algorithms for combinatorial optimization are usually designed in a specific way:
\begin{enumerate}
    \item Create a potential solution which is current best.
    \item Generate new solution. (usually in polynomial time) 
    \item If new solution better than current best, swap out the best with the newly generated. 
    \item If satisfied, output best solution, else continue.
\end{enumerate}

Both the Discrete Global-Best Harmony Search (DGHS), and Binary Multi-Scale Quantum Harmonic Oscillator (BMQHOA) have a similar framework.

\paragraph*{Discrete Global-Best Harmony Search Algorithm}
The Harmony Search algorithm (HS) was developed in 2001 \ref{geem et al} based on the improvisation of music players \textcolor{red}{EXPAND}. This algorithm was made for continuous search spaces and thus cannot be used for discrete search spaces in combinatorial optimization problems. The Discrete Global-Best harmony search was proposed by \textcolor{red}{EXPAND} to overcome this. The DGHS for KP01 also has a repair-operator and a greedy selection mechanism. A harmony in this algorithm refers to a candidate solution, or a certain selection of items to be put in the knapsack. 

It consists of 4 main parts:
\begin{enumerate}
    \item Create a harmony memory HM - a fixed size (HMS) of randomly generated harmonies. Initialize parameters harmony memory consider rate (HMCR), and pitch adjusting rate (PAR), and calculate profit-density vector for usage in the repair-operator.
    \item Create a new harmony using the current HM (\textcolor{red}{Algorithm x}).
    \item If generated harmony is better than the best in the harmony, replace it. If not, check if it is better than then worst harmony, and replace it if so. 
    \item If maximum number of iterations has been met, output the best sum profit value found so far.
\end{enumerate}

The parameters HMCR and PAR are used when generating new harmonies. They determine the likelihood of heading toward the current best harmony, and the likelihood of randomly flipping a decision variable. The idea is that current best harmony should have an influence on the generation of a new harmony, and that a decision variable (whether or not an item is put in the knapsack) should be flipped to allow for diversity in the HM, making it easier to overcome local maxima. These parameters are determined in terms of the current iteration like so:

\begin{equation}
    HMCR(t) = HMCR_{max} - \frac{HMCR_{max}-HMCR_{min}}{ITERATIONS} t
\end{equation}
\begin{equation}
    PAR(t) = PAR_{max} - \frac{PAR_{max}-PAR_{min}}{ITERATIONS} t
\end{equation}

The dynamic updating of the parameters is designed in this way to let larger values of HMCR help accelerate the convergence of the harmonies early on with the the help of the best individual harmony, while smaller values of HMCR can help you overcome local maxima. Similarly, larger values of PAR in the beginning allow for increases in diversity through mutations (item decision bit flips), when there is time to "explore" different variants, smaller values allow convergence at the end of the search. 

With these parameters one generates a new harmony:

\begin{algorithm}
\caption{Generating a new harmony during iterative part (part 2)}\label{harmonyGen}
    \begin{algorithmic}
        \For {$i = 1 \text{ to } noItems$}
            \If {$rand(0, 1) <= HMCR(t)$}
                \State $newHarmony[i] \gets bestHarmony[i]$ 
            \Else
                \State Generate a random integer number $a \in \{1, 2, ... HMS\}, a \neq best$
                \State $newHarmony[i] \gets Harmony_{a}[i]$ 
                \If {$rand(0, 1) <= PAR(t)$}
                    \State $newHarmony[i] = |newHarmony[i]-1|$ \Comment{// Flipping i'th item - mutation}
                \EndIf
            \EndIf
        \EndFor
    \end{algorithmic}
\end{algorithm}


Any time a harmony is generated, during Part 1 or Part 2, it is "repaired" in case the selection of items exceeds the capacity of the knapsack. The new generated harmony is also repaired if it can fit more items, without exceeding the capacity of the knapsack. The repair-operator consists of 2 phases:
\begin{enumerate}
    \item Drop phrase - repairing a harmony if it violates the constraint
    \item Add phase - adding a items into the knapsack if the total weight is less than the capacity
\end{enumerate}

The add phase is always done after the drop phase, as a harmony previously infeasible becomes feasible after the drop phase, but its total weight may be less than the capacity of the knapsack. Here is the pseudocode for the repair-operator:

\begin{algorithm}
\caption{Repair-operator after harmony generation}
    \begin{algorithmic}
        \State \Comment{// Profit-density vector precomputed during Part 1 (initialization)}
        \For {$i = 1 \text{ to } N$}
            \State $\lambda_{i} = \frac{profit_{i}}{weight_{i}}$
        \EndFor
        \State \Comment{Actual repair-operator starts here}
        \If {$totalWeight > maxWeight$}
        \State \Comment{// DROP phase}
        
        \EndIf
    \end{algorithmic}
\end{algorithm}

\paragraph*{Binary Multi-Scale Quantum Harmonic Oscillator Algorithm}

\paragraph{Pseudorandom numbers}


\section{Methodology}

Testdata is made up of different testcases, each of which is an input on which every algorithm is run.

\subsection{Generating testdata}

Five testcases were taken from a database \ref{some website}. These five testcases are mentioned/used several times in the literature.  Most of the testcases were made using a random number generator, where a random number was generated using the Mersenne Prime Twister (mt19937). A computer cannot generate a truly random number. A pseudorandom number is what is generated, which is a number which appears to be statistically random, but has been generated using a deterministic process. 

First, the number of solutions in memory (for the heuristic algorithms) was put in the input file (20 in all testcases and algorithms for this report). In all testcases, the number of items $n$ was chosen to either be 1000 or 100000. The profit and weight values were then randomly generated in a range of \, $ [1, n] $ while a sum-of-weights variable \emph{sum} was kept. The max weight was then randomly generated in a range of $[\frac{\text{sum}}{100} , \frac{\text{sum}}{10}]$, so that the knapsack wouldn't be able to hold all the items. To avoid any overflow errors, all generated numbers (and the sum of the profit/weight values) were kept below the max value of an \emph{int} which is $2^{31}-1$ in many programming languages.  Then $n$, $W$ (maxWeight), and each item (profit and weight values) was put in an input file. 

These input files were then saved for later testing of the algorithms. 

Each testcase was then run on each algorithm and saved in a spreadsheet:



 \bibliography{refs}

\end{document}
